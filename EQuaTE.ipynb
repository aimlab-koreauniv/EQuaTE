{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ac4f8ace-6141-4ccc-abb9-a9e57bf5fe39",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/.local/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2023-02-07 05:41:14.066489: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-02-07 05:41:14.947670: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "2023-02-07 05:41:14.947776: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "2023-02-07 05:41:14.947786: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/100-th training is finished.\n",
      "2/100-th training is finished.\n",
      "3/100-th training is finished.\n",
      "4/100-th training is finished.\n",
      "5/100-th training is finished.\n",
      "6/100-th training is finished.\n",
      "7/100-th training is finished.\n",
      "8/100-th training is finished.\n",
      "9/100-th training is finished.\n",
      "10/100-th training is finished.\n",
      "11/100-th training is finished.\n",
      "12/100-th training is finished.\n",
      "13/100-th training is finished.\n",
      "14/100-th training is finished.\n",
      "15/100-th training is finished.\n",
      "16/100-th training is finished.\n",
      "17/100-th training is finished.\n",
      "18/100-th training is finished.\n",
      "19/100-th training is finished.\n",
      "20/100-th training is finished.\n",
      "21/100-th training is finished.\n",
      "22/100-th training is finished.\n",
      "23/100-th training is finished.\n",
      "24/100-th training is finished.\n",
      "25/100-th training is finished.\n",
      "26/100-th training is finished.\n",
      "27/100-th training is finished.\n",
      "28/100-th training is finished.\n",
      "29/100-th training is finished.\n",
      "30/100-th training is finished.\n",
      "31/100-th training is finished.\n",
      "32/100-th training is finished.\n",
      "33/100-th training is finished.\n",
      "34/100-th training is finished.\n",
      "35/100-th training is finished.\n",
      "36/100-th training is finished.\n",
      "37/100-th training is finished.\n",
      "38/100-th training is finished.\n",
      "39/100-th training is finished.\n",
      "40/100-th training is finished.\n",
      "41/100-th training is finished.\n",
      "42/100-th training is finished.\n",
      "43/100-th training is finished.\n",
      "44/100-th training is finished.\n",
      "45/100-th training is finished.\n",
      "46/100-th training is finished.\n",
      "47/100-th training is finished.\n",
      "48/100-th training is finished.\n",
      "49/100-th training is finished.\n",
      "50/100-th training is finished.\n",
      "51/100-th training is finished.\n",
      "52/100-th training is finished.\n",
      "53/100-th training is finished.\n",
      "54/100-th training is finished.\n",
      "55/100-th training is finished.\n",
      "56/100-th training is finished.\n",
      "57/100-th training is finished.\n",
      "58/100-th training is finished.\n",
      "59/100-th training is finished.\n",
      "60/100-th training is finished.\n",
      "61/100-th training is finished.\n",
      "62/100-th training is finished.\n",
      "63/100-th training is finished.\n",
      "64/100-th training is finished.\n",
      "65/100-th training is finished.\n",
      "66/100-th training is finished.\n",
      "67/100-th training is finished.\n",
      "68/100-th training is finished.\n",
      "69/100-th training is finished.\n",
      "70/100-th training is finished.\n",
      "71/100-th training is finished.\n",
      "72/100-th training is finished.\n",
      "73/100-th training is finished.\n",
      "74/100-th training is finished.\n",
      "75/100-th training is finished.\n",
      "76/100-th training is finished.\n",
      "77/100-th training is finished.\n",
      "78/100-th training is finished.\n",
      "79/100-th training is finished.\n",
      "80/100-th training is finished.\n",
      "81/100-th training is finished.\n",
      "82/100-th training is finished.\n",
      "83/100-th training is finished.\n",
      "84/100-th training is finished.\n",
      "85/100-th training is finished.\n",
      "86/100-th training is finished.\n",
      "87/100-th training is finished.\n",
      "88/100-th training is finished.\n",
      "89/100-th training is finished.\n",
      "90/100-th training is finished.\n",
      "91/100-th training is finished.\n",
      "92/100-th training is finished.\n",
      "93/100-th training is finished.\n",
      "94/100-th training is finished.\n",
      "95/100-th training is finished.\n",
      "96/100-th training is finished.\n",
      "97/100-th training is finished.\n",
      "98/100-th training is finished.\n",
      "99/100-th training is finished.\n",
      "100/100-th training is finished.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torchquantum as tq\n",
    "import random\n",
    "import os \n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from collections import OrderedDict\n",
    "from torchquantum.encoding import encoder_op_list_name_dict\n",
    "from torchquantum.layers import U3CU3Layer0, RandomLayer\n",
    "from torchvision import datasets, transforms\n",
    "from torch.distributions.bernoulli import Bernoulli\n",
    "from torchquantum.encoding import encoder_op_list_name_dict as enc_dict\n",
    "from torchquantum.layers import U3CU3Layer0 \n",
    "from models import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from datetime import datetime\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"2\"\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "num_client  = 10\n",
    "\n",
    "num_class = 4\n",
    "EPOCH = 100\n",
    "TrainLoader = DataLoader(Dataset(np.load('data/mnist/train_x.npy')[:18623],np.load('data/mnist/train_y.npy')[:18623],0),batch_size=256,shuffle=True)\n",
    "\n",
    "Data = []\n",
    "Labels = []\n",
    "for i, (data, labels) in enumerate(TrainLoader):\n",
    "    Data.append(data)\n",
    "    Labels.append(labels)\n",
    "    if i==2:\n",
    "        break\n",
    "        \n",
    "TrainLoader = DataLoader(Dataset(Data[0],Labels[0],0),batch_size=32,shuffle=True)\n",
    "TestLoader  = DataLoader(Dataset(Data[1],Labels[1],0),batch_size=32,shuffle=True)\n",
    "ValidLoader = DataLoader(Dataset(Data[2][:32],Labels[2][:32],0), batch_size=1, shuffle=True)\n",
    "device = torch.device('cuda:0')\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "q_device = tq.QuantumDevice(n_wires=4).to(device)\n",
    "\n",
    "class QNN(tq.QuantumModule): # Target QML code\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.n_wires = 4\n",
    "        self.encoder = tq.GeneralEncoder(enc_dict['4x4_ryzxy'])\n",
    "        self.pqc     = tq.RandomLayer(n_ops=50, wires=[0,1,2,3])\n",
    "        \n",
    "    def forward(self, x,q_device=q_device):\n",
    "        batchsize = x.shape[0]\n",
    "        x = x.reshape(batchsize,-1).to(dtype=torch.complex64)\n",
    "        self.encoder(q_device , x)\n",
    "        self.pqc(q_device)\n",
    "        x = tq.expval(q_device,\n",
    "                      [i for i in range(num_class)], \n",
    "                      [tq.PauliZ() for _ in range(num_class)]\n",
    "                     ).squeeze() \n",
    "        return x\n",
    "    \n",
    "model = QNN().to(device)\n",
    "opt   = torch.optim.Adam(model.parameters(), lr=5e-3)\n",
    "\n",
    "today = datetime.today().strftime(\"%m%d%H%M%S\")\n",
    "writer = SummaryWriter(f'runs/{today}')\n",
    "\n",
    "def train(ep,\n",
    "          train_loader,\n",
    "          test_loader,\n",
    "          valid_loader, \n",
    "          model, \n",
    "          device, \n",
    "          criterion,\n",
    "          optimizer):\n",
    "    \n",
    "    Train_Loss = 0 \n",
    "    Test_Acc   = 0\n",
    "    # Train #\n",
    "    for niter, (data, labels) in enumerate(train_loader):\n",
    "        inputs  = data.to(device,dtype=torch.float32)\n",
    "        targets = labels.to(device,dtype=torch.long)\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        loss    = criterion(torch.softmax(outputs,dim=-1), targets)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        Train_Loss = loss.item()\n",
    "        \n",
    "    # Test #\n",
    "    with torch.no_grad():\n",
    "        Size = 0\n",
    "        Corrects = 0\n",
    "        for _, (x,y) in enumerate(test_loader):\n",
    "            x = x.to(device,dtype=torch.float32)\n",
    "            y = y.to(device,dtype=torch.long)\n",
    "            y_hat = model(x) \n",
    "            _, indices = y_hat.topk(1, dim=1)\n",
    "            masks = indices.eq(y.view(-1, 1).expand_as(indices))\n",
    "            Size += y.shape[0]\n",
    "            Corrects += masks.sum().item()\n",
    "        Test_Acc = Corrects / Size\n",
    "            \n",
    "    # Barren Plateaus # \n",
    "    grad_bp,mean, var = {},{},{}\n",
    "    \n",
    "    for i,(name, params) in enumerate(model.pqc.named_parameters()):\n",
    "        grad_bp[name] = []\n",
    "    \n",
    "    for niter, (data, labels) in enumerate(valid_loader):\n",
    "        inputs  = data.to(device,dtype=torch.float32)\n",
    "        targets = labels.to(device,dtype=torch.long)\n",
    "        outputs = model(inputs)\n",
    "        loss    = criterion(torch.softmax(outputs,dim=-1).unsqueeze(0), targets)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        for i, (name, params) in enumerate(model.pqc.named_parameters()):\n",
    "            grad_bp[name].append(params.grad.clone().detach().cpu().numpy())\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        if niter==31:\n",
    "            break\n",
    "\n",
    "    \n",
    "    for key in grad_bp.keys():\n",
    "        grads     = grad_bp[key]\n",
    "        grads     = np.array(grads)\n",
    "        mean[key] = np.mean(grads)\n",
    "        var[key]  = np.var(grads)\n",
    "    \n",
    "    return Train_Loss, Test_Acc, var\n",
    "\n",
    "def Helper(ep,var):\n",
    "    Event = []\n",
    "    for key in  var.keys():\n",
    "        gate  = key.split('.')[2].split('_params')[0]\n",
    "        order = int(key.split('.')[1]) + 1\n",
    "        bp_value = var[key]\n",
    "        if bp_value <= 1e-5:\n",
    "            event = f\"[Epoch {ep}] {order}-th params ({gate} Gate) has barren plateaus (BP value: {var[key]})\"\n",
    "            Event.append(event)\n",
    "    return '<br>'.join(Event)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    for ep in range(EPOCH):\n",
    "        Train_Loss, Test_Acc, var = train(  ep, \n",
    "                                            TrainLoader, \n",
    "                                            TestLoader, \n",
    "                                            ValidLoader,  \n",
    "                                            model,  \n",
    "                                            device,  \n",
    "                                            criterion, \n",
    "                                            opt\n",
    "                                         )\n",
    "\n",
    "        print(f\"{ep+1}/{EPOCH}-th training is finished.\")\n",
    "\n",
    "        writer.add_scalars(f'Metric/Loss', {'loss': Train_Loss} ,ep+1)\n",
    "        writer.add_scalars(f'Metric/Accuracy', {'acc': Test_Acc} ,ep+1)\n",
    "        writer.add_scalars(f'Metric/BarrenPlateaus', var,ep+1)\n",
    "        writer.add_text('Event',Helper(ep+1,var), ep+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7fed1df-bddc-4d31-bc90-2388f2bdbe93",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
